{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8D395kTdLz5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semiconductor Data Classification"
      ],
      "metadata": {
        "id": "9eHhyb0ZL2za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import and Explore the Data\n",
        "Let's start by importing the necessary libraries and loading the dataset."
      ],
      "metadata": {
        "id": "VuhoHjw1L3vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('sensor-data.csv')\n",
        "\n",
        "# Explore the data\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "id": "vanp_KCNL7qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Cleansing\n",
        "We'll handle missing values, drop any irrelevant columns, and ensure that the data is clean for further analysis."
      ],
      "metadata": {
        "id": "SJebKRDGL-ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Drop columns with excessive missing values (if any) or apply imputation\n",
        "# For example, drop a column:\n",
        "# df.drop(columns=['column_name'], inplace=True)\n",
        "\n",
        "# For this example, we'll fill missing values with the median\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Check for any non-numeric columns that might need conversion\n",
        "print(df.select_dtypes(include=['object']).columns)\n"
      ],
      "metadata": {
        "id": "VUaVYyPWMCGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Data Analysis & Visualization\n",
        "Performing statistical analysis and visualizations to understand the data better."
      ],
      "metadata": {
        "id": "07weUcwwMDdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Univariate analysis: distribution of target variable\n",
        "sns.countplot(df['target_column'])\n",
        "plt.show()\n",
        "\n",
        "# Bivariate analysis: correlation heatmap\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr, cmap='coolwarm', annot=False)\n",
        "plt.show()\n",
        "\n",
        "# More advanced plots can be added here based on insights.\n"
      ],
      "metadata": {
        "id": "qIZz6S6wMIfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Data Pre-processing\n",
        "Segregate the data into features and target variables, handle target balancing, and split into training and testing sets."
      ],
      "metadata": {
        "id": "K8sF2jsfMJXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Segregate predictors vs target attributes\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# Check for target imbalance\n",
        "print(y.value_counts())\n",
        "\n",
        "# If the target is imbalanced, apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Check for similarity in statistical characteristics\n",
        "print(\"Training data stats:\")\n",
        "print(pd.DataFrame(X_train).describe())\n",
        "print(\"Test data stats:\")\n",
        "print(pd.DataFrame(X_test).describe())\n"
      ],
      "metadata": {
        "id": "rVmxXe2RMMWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Training, Testing, and Tuning\n",
        "Train different models, evaluate them, and fine-tune for the best results."
      ],
      "metadata": {
        "id": "SOlxsiq4MOQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1: Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest Classifier Report\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Model 2: Support Vector Machine\n",
        "svc = SVC(random_state=42)\n",
        "svc.fit(X_train, y_train)\n",
        "y_pred_svc = svc.predict(X_test)\n",
        "print(\"Support Vector Machine Report\")\n",
        "print(classification_report(y_test, y_pred_svc))\n",
        "\n",
        "# Model 3: Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "print(\"Naive Bayes Report\")\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "\n",
        "# Model comparison\n",
        "models = ['Random Forest', 'SVM', 'Naive Bayes']\n",
        "train_accuracies = [cross_val_score(rf, X_train, y_train, cv=5).mean(),\n",
        "                    cross_val_score(svc, X_train, y_train, cv=5).mean(),\n",
        "                    cross_val_score(nb, X_train, y_train, cv=5).mean()]\n",
        "\n",
        "test_accuracies = [rf.score(X_test, y_test),\n",
        "                   svc.score(X_test, y_test),\n",
        "                   nb.score(X_test, y_test)]\n",
        "\n",
        "# Display the accuracies\n",
        "print(\"Train Accuracies:\", train_accuracies)\n",
        "print(\"Test Accuracies:\", test_accuracies)\n",
        "\n",
        "# Choose the best model\n",
        "best_model = models[np.argmax(test_accuracies)]\n",
        "print(f\"The best model is: {best_model}\")\n",
        "\n",
        "# Save the model for future use\n",
        "import joblib\n",
        "joblib.dump(rf, 'best_model.pkl')\n"
      ],
      "metadata": {
        "id": "tK8Da83XMQT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Conclusion and Improvisation\n",
        "Finally, we summarize the findings and suggest possible improvements."
      ],
      "metadata": {
        "id": "UeEq41k-MTL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Random Forest model gave the best accuracy on the test data, indicating that it's well-suited for this classification problem. Other models like SVM and Naive Bayes also performed adequately, but the Random Forest outperformed them. Future work could involve exploring deeper hyperparameter tuning, trying ensemble methods, or incorporating more advanced feature engineering techniques to further boost accuracy.\")\n"
      ],
      "metadata": {
        "id": "M0AOHbDnMVPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}